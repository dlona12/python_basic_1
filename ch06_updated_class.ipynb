{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 데이터 로딩과 저장, 파일 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"figure\", figsize=(10, 6))\n",
    "pd.options.display.max_colwidth = 75\n",
    "pd.options.display.max_columns = 20\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "plt.rcParams[\"font.family\"] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. csv 텍스트 데이터\n",
    "## 1.1 csv파일 읽기\n",
    "* 판다스에서 CSV 파일을 읽기: pd.read_csv(path, encoding = 'utf-8') 함수를 사용\n",
    "    * path 인수 : 파일시스템에서의 위치, URL, 파일객체를 나타내는 문자열\n",
    "    * encoding 인수 - 유니코드 인코딩 종류을 지정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) header(컬럼명)가 있는 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat examples/ex1.csv\n",
    "\n",
    "df = pd.read_csv(\"examples/ex1.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2) header가 없는 파일 : 컬럼을 지정\n",
    "        * names 인수 - 열 이름 리스트\n",
    "        * header 인수 - 헤더가 없을 경우 None으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"examples/ex2.csv\")\n",
    "\n",
    "names = [\"a\", \"b\", \"c\", \"d\", \"message\"]\n",
    "pd.read_csv(\"examples/ex2.csv\", names=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3) 계층적 색인 지정하기\n",
    "        * index_col 인수 - 색인으로 사용할 열 번호나 이름, 계층적 색인을 지정할 경우 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = pd.read_csv(\"examples/csv_mindex.csv\",\n",
    "                     index_col=[\"key1\", \"key2\"])\n",
    "parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4) 여러 개의 공백으로 필드를 구분한 파일 \n",
    "        * sep 인수 - 필드을 구분하기 위해 사용할 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"examples/ex3.txt\", sep=\"\\s+\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5) 주석을 갖고 있는 파일\n",
    "        * skiprows 인수 - 파일의 시작부터 무시할 행 수 또는 무시할 행 번호가 담긴 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"examples/ex4.csv\", skiprows=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6) 누락된 값이 있는 파일 : 비어 있는 문자열, NA, NULL을 NaN으로 출력 \n",
    "        * na_values 인수 - NA 값으로 처리할 값들의 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(\"examples/ex5.csv\")\n",
    "result\n",
    "# 1) 누락된 값을 확인 : isna()함수 이용\n",
    "pd.isna(result)\n",
    "\n",
    "# 2) na_values -> 컬럼별 문자열 집합을 받아서 누락된 값 처리\n",
    "sentinels = {\"message\": [\"foo\", \"NA\"], \"something\": [\"two\"]}\n",
    "result = pd.read_csv(\"examples/ex5.csv\", na_values=sentinels)\n",
    "print(result)\n",
    "result.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7) 텍스트 파일 조금씩 \n",
    "        * nrows 인수 - 파일의 첫 일부만 읽어올 때 처음 몇 줄을 읽을 것인지 지정 \n",
    "        * chunksize 인수 - TextParser 객체에서 사용할 한 번에 익을 파일의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = 10\n",
    "result = pd.read_csv(\"examples/ex6.csv\")\n",
    "\n",
    "result = pd.read_csv(\"examples/ex6.csv\", nrows=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제]  ex6.csv 파일을 순회하면서 \"key\"열의 값을 세기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat examples/ex6.csv\n",
    "chunker = pd.read_csv(\"examples/ex6.csv\", chunksize=1000)\n",
    "\n",
    "tot = pd.Series([], dtype='int64') #Series 객체 생성\n",
    "\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece[\"key\"].value_counts(), fill_value=0) \n",
    "\n",
    "tot = tot.sort_values(ascending=False)\n",
    "tot\n",
    "# tot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. csv 텍스트 파일 저장하기\n",
    " 판다스에서 csv파일을 저장하기 : df.to_csv(path, encoding = 'utf-8')함수 사용\n",
    "path : 파일 시스템에서의 위치,url,파일객체를 나타내는 문자열\n",
    "encoding : 유니코드 인코딩 종류를 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"examples/ex5.csv\")\n",
    "data # dataframe에 데이터 저장\n",
    "data1 = data[['a','b','d']] # 인덱싱 : 데이터 프레임의 일부 데이터만 추출해서 데이터프레임 생성 \n",
    "data1\n",
    "data1.to_csv(\"examples/out.csv\") # 데이터프레임.to_csv(저장할 파일명)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. HTML 데이터\n",
    "## 2.1 웹 스크래핑 - read_html()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pd.read_html() - html 문서 내 테이블 데이터 읽어 테이블 구조의 모든 데이터를 데이터프레임으로 저장해서 반환\n",
    "    * https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [예제] 미국 예금 보험 공사에서 부도 은행을 보여주는 HTML 문서 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tables = pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")\n",
    "print(type(tables))\n",
    "# len(tables)\n",
    "\n",
    "failures = tables[0] # 인덱싱 \n",
    "# print(type(failures))\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 컬럼명 수정하기\n",
    "# failures.columns\n",
    "failures.columns = ['BankNameBank', 'CityCity', 'StateSt', 'CertCert',\n",
    "       'AcquiringInstitutionAI', 'ClosingDate', 'FundFund']\n",
    "failures.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 데이터 변환 : 시간/날짜 문자열 -> datetime 객체 변환\n",
    "close_timestamps = pd.to_datetime(failures[\"ClosingDate\"])\n",
    "close_timestamps.head()\n",
    "#3) 년도별 데이터 개수 추출\n",
    "close_timestamps.dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [실습] : 네이버 주식 데이터 가져오기 - read_html()\n",
    "* https://www.naver.com/ -> 증권 -> 국내증시 -> 시가총액\n",
    "    * https://finance.naver.com/sise/sise_market_sum.naver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html5lib  lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 'euc-kr' 또는 'cp949'로 인코딩 설정하여 페이지 읽기 시도\n",
    "df_list = pd.read_html(\"https://finance.naver.com/sise/sise_market_sum.naver\",encoding='cp949')\n",
    "type(df_list)\n",
    "len(df_list)\n",
    "# 데이터프레임 확인\n",
    "# for df in df_list:\n",
    "#     print(df)\n",
    "df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임 탐색\n",
    "df = df_list[1]\n",
    "# 결측치 처리 - 종목명\n",
    "# 조건색인을 이용해서 불리언 인덱싱으로 새로운 데이터프레임 생성\n",
    "#1. 조건 색인\n",
    "df['종목명'].notnull() # notna()\n",
    "#2. 조건색인을 데이터프레임에 넣어주기 -true 값만 추출 \n",
    "cond = df['종목명'].notnull()\n",
    "data = df[cond]\n",
    "data\n",
    "data.head()\n",
    "# 최종 결과를 데이터프레임 형태로 저장 - df.to_csv(파일명)\n",
    "data.to_csv(\"Naver_kospi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 웹 스크래핑 - requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 웹 페이지 정보 가져오기 - requests 패키지 이용\n",
    "  * requests.get()\n",
    "  * requests.get().status_code\n",
    "  * requests.get().text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. requests 모듈 설치하기\n",
    "# !pip install requests\n",
    "# 2. requests 모듈 임포트하기\n",
    "import requests\n",
    "#3. url 가져오기\n",
    "url = \"https://api.github.com/repos/pandas-dev/pandas/issues\"\n",
    "resp = requests.get(url)\n",
    "print(resp.status_code)\n",
    "#4. 웹페이지 내용 텍스트 확인\n",
    "resp.text\n",
    "#원하는 데이터 처리 방법 \n",
    "# 5. json 내용을 딕셔너리로 변환\n",
    "data = resp.json() # json의 내용을 딕셔너리 형태로 변환한 객체을 반환\n",
    "type(data)\n",
    "len(data)\n",
    "#6. json 내용 확인 \n",
    "data[0]\n",
    "#7. json의 딕셔너리\n",
    "data[0].keys() \n",
    "# # 8. 딕셔너리를 DataFrame으로 생성하기\n",
    "issues = pd.DataFrame(data, columns=[\"number\", \"title\",\n",
    "                                      \"labels\", \"state\"])\n",
    "issues.head()\n",
    "# 데이터프레임을 저장\n",
    "issues.to_csv(\"pandas_issues.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 블로그의 키워드 관련 검색 결과 텍스트 데이터 가져오기 - requests 모듈 사용시 한계점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# url = \"https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=ALL&orderBy=sim&keyword=%ED%8C%8C%EC%9D%B4%EC%8D%AC\"\n",
    "\n",
    "params = {\n",
    "    'pageNo' : 1,\n",
    "    'rangeType' : 'ALL',\n",
    "    'orderBy' : 'sim',\n",
    "    'keyword' : '파이썬'\n",
    "}\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "response = requests.get('https://section.blog.naver.com/Search/Post.naver?', params=params)\n",
    "\n",
    "print(response.status_code) # HTTP GET 방식으로 URL 파라미터 값 전달\n",
    "print(response.url) # status_code 는 응답코드\n",
    "print(response.text) # text에는 HTML 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 웹 스크래핑 - beautifulsoup\n",
    "* 웹 페이지 정보 추출하기 - beautifulsoup 사용법 \n",
    "  * soup.select_one()\n",
    "  * soup.select()\n",
    "  * get_text()\n",
    "* 개발자 도구 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] 네이버 지식iN의 검색어 관련 검색 결과 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://kin.naver.com/search/list.naver?query=%ED%8C%8C%EC%9D%B4%EC%8D%AC'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html = response.text\n",
    "    # 웹 페이지 정보를 원하는 데이터 정보 추출\n",
    "    #1. html -> beautifulsoup 객체로 변환\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #2. 첫 번째 제목 한개 선택하여 텍스트 가져오기\n",
    "    # title = soup.select_one('#s_content > div.section > ul > li:nth-child(1) > dl > dt > a')\n",
    "    # print(title.get_text())\n",
    "\n",
    "    # 첫 번째 웹페이지의 모든 제목들을 선택하여 텍스트 리스트 가져오기\n",
    "    titles = soup.select('#s_content > div.section > ul > li:nth-child(n) > dl > dt > a')\n",
    "    print(type(titles))\n",
    "    for title in titles:\n",
    "        print(title.get_text()) # 제목 텍스트 가져오기\n",
    "else : \n",
    "    print(response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 웹 스크래핑 - Selenium을 이용하여 사이트 자동화하기\n",
    "* selenium 사용법\n",
    "  1. 시스템의 OS 및 Chrome 브라우저의 버전 확인\n",
    "  2. Chrome 드라이버 다운로드 및 설치\n",
    "  3. 크롬 드라이버을 프로젝트 폴더에 저장 - 크롬 드라이버의 설치 경로\n",
    "     * from selenium import webdriver\n",
    "     * driver = webdriver.Chrome()\n",
    "* 동적으로 웹 페이지 정보 추출하기\n",
    "     * driver.get(\"웹주소\") \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [예제] 크롬 드라이버를 이용하여 타겟 웹사이트 실행하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\김수지\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to obtain driver for chrome using Selenium Manager.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mSeleniumManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:106\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m    104\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m--> 106\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m browser_path \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowser_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:154\u001b[0m, in \u001b[0;36mSeleniumManager.run\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m completed_proc\u001b[38;5;241m.\u001b[39mreturncode:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WebDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsuccessful command executed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstderr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: Unsuccessful command executed: C:\\Users\\김수지\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\windows\\selenium-manager.exe --browser chrome --language-binding python --output json.\n{'code': 69, 'message': 'Driver unavailable: Driver path: ', 'driver_path': '', 'browser_path': ''}\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Chrome 브라우저 실행\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 타켓 웹페이지 열기\u001b[39;00m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrowser_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvendor_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:41\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate or obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to obtain driver for chrome using Selenium Manager.; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "# Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome()\n",
    "# 타켓 웹페이지 열기\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [문제] Selenium을 이용하여 타겟 웹사이트 실행하기 /검색어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# 1. Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "#2. 타켓 웹페이지 열기\n",
    "url = \"https://www.google.com\"\n",
    "driver.get(url)\n",
    "\n",
    "# 페이지가 완전히 로드될 때까지 2초 대기\n",
    "time.sleep(2)\n",
    "\n",
    "try:\n",
    "    # 1. 검색 상자 요소 찾기\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "\n",
    "    # 2. 검색어 입력\n",
    "    search_query = \"OpenAI\"\n",
    "    search_box.send_keys(search_query)\n",
    "\n",
    "    # 3. Enter 키를 눌러 검색 수행\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    # 검색 결과가 로드될 때까지 5초 대기\n",
    "    time.sleep(5)\n",
    "\n",
    "    # 4. 검색 결과 추출\n",
    "    search_results = driver.find_elements(By.CSS_SELECTOR, \"div.g\")\n",
    "\n",
    "    # 검색 결과의 제목(<h3></h3>)과 URL(<a href=\"...\"></a>) 인쇄\n",
    "    for result in search_results:\n",
    "        title = result.find_element(By.CSS_SELECTOR, \"h3\").text\n",
    "        url = result.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "        print(\"제목:\", title)\n",
    "        print(\"URL:\", url)\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"오류가 발생했습니다:\", e)\n",
    "\n",
    "# finally:\n",
    "#     # 브라우저 창 닫기\n",
    "#     driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [응용문제] : 시가총액이 높은 상위 500개 기업의 데이터를 가져오기\n",
    "* 1~50 페이지 중 10개 페이지만 데이터 읽어오기 \n",
    "* 10개 페이지를 하나의 데이터로 합치기\n",
    "* 위 결과를 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Chrome 브라우저를 열고 웹 페이지로 이동\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 시가 총액 상위 500개 기업 데이터를 가져올 url\n",
    "url = \"https://finance.naver.com/sise/sise_market_sum.naver?&page=\"\n",
    "\n",
    "#10개 페이지의 데이터를 저장할 리스트\n",
    "kospi500 = []\n",
    "\n",
    "#10개 페이지에 대해 반복하여 데이터 수집\n",
    "for num in range(1, 11): # 페이지 번호 제어\n",
    "    full_url = url +str(num)\n",
    "    driver.get(full_url) # 페이지로 이동\n",
    "    time.sleep(2) #페이지가 로드될 때까지 기다림\n",
    "    html = driver.page_source #페이지의 html가져오기\n",
    "    data = pd.read_html(html) #html을 데이터프레임으로 읽기 시도\n",
    "    df = data[1] # 두 번째 테이블에 종목 데이터가 있음\n",
    "    df = df[df['종목명'].notnull()] #조건 색인으로 결측치 처리\n",
    "    kospi500.append(df) #선택된 데이터를 리스트에 추가\n",
    "\n",
    "#10개 패이지의 데이터를 하나의 데이터프레임으로 합치기\n",
    "combined_df = pd.concat(kospi500)\n",
    "\n",
    "# 결과 출력\n",
    "combined_df .to_csv(\"combined_kospi.csv\")\n",
    "\n",
    "# #드라이버 닫기 \n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeutifulSoup\n",
    "import pandas as ppd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome 웹 드라이버 실행\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#페이지 로딩 시간\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targer 웹페이지 url - 사전 작업 \n",
    "# 삼성전자 일별 시세 정보 페이지\n",
    "samsung_url = \"https://finance.naver.com/item/side_day.naver?code=005930\"\n",
    "\n",
    "#페이지 요청\n",
    "driver.get(samsung_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 웹 페이지의 html 가져옹기\n",
    "    html = driver.page_source\n",
    "    df = pd.read_html\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(\"Network Error\")\n",
    "finally:\n",
    "    driver.quit(()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
